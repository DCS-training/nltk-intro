{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Natural Language Toolkit (NLTK)\n",
    "\n",
    "## Text Analysis with NLTK\n",
    "\n",
    "### [Centre for Data, Culture & Society](http://cdcs.ed.ac.uk)\n",
    "\n",
    "Course Instructor: Xandra Dave Cochran\n",
    "\n",
    "Course Dates: February 2024\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Refresher\n",
    "\n",
    "Can you name some of Python's **data types**?\n",
    "\n",
    "There are two data types that are particularly important to understand when doing text analysis: **string** and **list**.  \n",
    "\n",
    "**Strings** are sequences of characters (as in letters, numbers, spaces, or punctuation marks) that are contained within single quotes (`'I am a string'`) or double quotes (`\"I'm a string too!\"`).\n",
    "\n",
    "**Lists** are collections of data of any type (str, int, float, dict, list, tuple, set) that are contained within square brackets (`[ ]`).  Lists can have zero or more elements, and a single list can contain elements of different data types (for example: `['hi', 123, \"bonjour !\", [], {\"A\":1, \"B\":2}, (1.5, 2.8)]`).  The elements inside lists are *ordered*, which allows us to reference them by their *index* (position - numeric, starts from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world\")   # print() is a FUNCTION, \"Hello world\" is a STRING (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = \"Hello world\"  # we've assigned the string to a VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment.  Comments are for you and me, the computer knows to ignore them.\n",
    "# In this comment I'll explain that to return values, we can choose to use print() or \n",
    "# not, and the output will be slightly different, as you can see below.\n",
    "# If you want to show multiple values below a code cell like this one, you'll need all\n",
    "# of the values you want returned inside a print() function (except the last one, which\n",
    "# is optional), otherwise only the value in the last line of your cell will be shown\n",
    "print(greeting)\n",
    "greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_list = [\"hello\", \"bonjour\", \"salaam\", \"hola\"]  # 1-dimensional list of strings\n",
    "print(type(greeting_list))                              # type() is another function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a **for** loop or a **while** loop to iterate through items in a collection, such as our `greeting_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for greeting in greeting_list:\n",
    "    print(\"Greeting at index \"+str(i)+\": \"+greeting)  # an example of CONCATENTATION \n",
    "    i += 1                                            # a shortcut for writing i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "while i > -5:\n",
    "    print(greeting_list[i])  # we can reference indeces backwards using negative numbers\n",
    "    i = i - 1                # we can't write i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access subsets of a list using **slicing** with square brackets and colons, where the number before the colon is included in the slice but the number after the colon is not.  If a number is omitted before the colon, Python knows to go all the way to the starting element of the list.  If a number is omitted after the colon, Python knows to go all the way to the ending element of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greeting_list[0:1])\n",
    "print(greeting_list[:1])\n",
    "print(greeting_list[2:])\n",
    "print(greeting_list[:])\n",
    "print(greeting_list[:-3])\n",
    "print(greeting_list[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The demos below reference:\n",
    "\n",
    "Steven Bird, Ewan Klein, Edward Loper (2019) *Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit.*  3rd Edition.  https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1\n",
    "Import the nltk library and download the texts it comes pre-loaded with (It will suggest saving it to your home directory - ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"Marianne\", lines=20)  # default lines listed is 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2.concordance (\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.similar(X)` will output words that appear in the text surrounded by similar words to `X`.  This means that the output is likely to include words that are the same part of speech as `X` and words that are synonyms of `X`.  For example, the words that occur in similar contexts to the noun `\"happiness\"` are also nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2.similar(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.common_contexts(L, N)` will output words that appear immediately to the left and right of all input words in list `L`.  For example, `\"such\"` and `\"of\"` are found surrounding both `\"kindness\"` and `\"happiness\"` in text2, so they are included in the output of `.common_contexts([\"kindness\", \"happiness\"])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"kindness\", lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"happiness\", lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"], num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"], num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"])  # defaults to 20 maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no result is found, NLTK will output a message that tells you this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the frequency of a list of words (of any length) as they occur from the beginning through the end of the text, you can use a Lexical Dispersion Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.dispersion_plot([\"happiness\", \"fortune\", \"anxiety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = len(text1)\n",
    "ss = len(text2)\n",
    "print(\"Length of Moby Dick:\", md)\n",
    "print(\"Length of Sense and Sensibility:\", ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_vocab = len(set(text1))\n",
    "ss_vocab = len(set(text2))\n",
    "print(\"Length of vocabulary (unique words) of Moby Dick:\", md_vocab)\n",
    "print(\"Length of vocabulary (unique words) of Sense and Sensibility:\", ss_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure the diversity of word choice, \n",
    "# or lexical diversity:\n",
    "def lexicalDiversity(t):\n",
    "    return len(set(t))/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lexical Diversity of Moby Dick:\", lexicalDiversity(text1))\n",
    "print(\"Lexical Diversity of Sense and Sensibility:\", lexicalDiversity(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis with NLTK\n",
    "\n",
    "****\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "Steven Bird, Ewan Klein, Edward Loper (2019) *Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit.*  3rd Edition.  https://www.nltk.org/book/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK, which stands for Natural Language Toolkit, is a popular coding library for text analysis with the programming language Python.  While Python alone has some basic capabilities for analyzing text, NLTK has much more to offer, as we will see!  This Jupyter Notebook will cover the following concepts:\n",
    "\n",
    "* [Tokenization](#tok)\n",
    "* [Frequency counts and distributions](#fre)\n",
    "* [Normalization](#nor)\n",
    "* [Stemming](#ste)\n",
    "* [Lemmatizing](#lem)\n",
    "* [Part-of-speech tagging](#pos)\n",
    "* [Collocations and n-grams](#col)\n",
    "\n",
    "These are the building blocks for more complicated text analysis tasks.  They are generally part of the first step of text analysis called **preprocessing**, which gets your text data formatted in a way that NLTK's methods and functions can easily interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin our text analysis, though, we should import the libraries we'll want to use to explore the capabilities of NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import * # the `*` means import all corpora (you could also specify a specific corpus)\n",
    "\n",
    "# As an alternative to `nltk.download()`, as shown in last Notebook's class, you can specify what\n",
    "# packages from NLTK to download\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt\n",
    "\n",
    "import matplotlib.pyplot as plt   # for drawing charts to visualize data\n",
    "\n",
    "import re               # the Regular Expression (or RegEx) library, which is useful in combination with NLTK\n",
    "import string           # another useful library for acccessing lists of all letters, all punctuation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the names on the left (`text1`, `text2`, ..., `text9`) are **variables** that point to the text corpora on the right (`Moby Dick...`, `Sense and Senibility...`, ..., `The Man Who Was Thursday...`).\n",
    "\n",
    "I wonder what happens if we try to print one of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.  That doesn't show us much.  What if we try slicing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"tok\"></a>\n",
    "You can see that the text has been **tokenized**, or separated into its individual words, numbers, and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing works for us to view **tokens** in the NLTK Text object, which is the data type of `text2`.  Let's try out some of the methods and functions specific to NLTK, though, as they are designed for working with Text objects!\n",
    "\n",
    "Let's begin with the `concordance()` method.  We pass a single **token**, as a **string** (type=str), into this method.  What token would you like to see in its different contexts within the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('opinion')  # optional parameter: lines=20 (or any number you choose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `concordance()` method shows 25 contexts in which the input word is used, but you can specify how many contexts you would like to see by saying something like: `text2.concordance('opinion', lines=63`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('opinion', lines=100)  # it's okay to input a greater number of lines than there are matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about the *context* of words (or tokens!) in text analysis, we're referring to the surrounding words of a given word.  Concordances show a bit of context to the left of an input word (just before the word appears) and to the right of that word (just after that word appeared). \n",
    "\n",
    "The words \"good\" and \"opinion\" seem to occur together quite a bit!  To see the other words appear near that pair, we can use the `common_contexts()` method.  We pass a **list** of tokens (each token as a string) into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"good\", \"opinion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that \"the good opinion of\" is the complete phrase in which the pair of words, \"good\" and \"opinion\", appear together in this text.  They don't occur together in other contexts!  But what about individually?\n",
    "\n",
    "We can use the **similar** method to see words that appear in similar contexts, meaning they're surrounded by similar tokens, as the token we input.  Note that we pass in a single token as a string to this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words with a similar context as 'good':\")\n",
    "text2.similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words with a similar context as 'opinion':\")\n",
    "text2.similar(\"opinion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of words that occur together, such as \"good\" and \"opinion,\" are referred to as **bigrams**, where \"bi\" indicates two.  **N-grams** are groups of words that occur together, where n is a number of your choice.\n",
    "\n",
    "To get all the bigrams in a text, we can use the `bigrams()` method, into which we pass the variable referring to the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_list = list(nltk.bigrams(text2))\n",
    "print(bigrams_list[:100])  # prints the first 100 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class we looked quickly at a **dispersion plot**, which is a chart that visualizes when particular tokens appear within a text.  Let's try making another one of those.  We pass in a list of individual tokens, where each token is a string, to make a dispersion plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character names (note that they can't be multiple words, or there won't be a match)\n",
    "text2.dispersion_plot([\"Marianne\", \"Elinor\", \"Edward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this plot reflect the text?\n",
    "\n",
    "Marianne and Elinor are the main characters of the book 'Sense and Sensibility,' so it makes sense that we'd see them consistently throughout the text!  Edward is a supporting character, so we see that his name occurs less frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another text.  NLTK includes some books that were digitized for [Project Gutenberg](https://www.gutenberg.org).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of those books, Alice's Adventures in Wonderland (with the fileid `carroll-alice.txt`), to practice tokenizing on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw = nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")\n",
    "print(type(aiw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize the string of Alice's Adventures in Wonderland to split it into individual words and punctuation using the function `word_tokenize()`.  We can split the string into individual sentences using the function `sent_tokenize()`.  Both tokenization functions output a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_tokens = word_tokenize(aiw)\n",
    "print(\"Total tokens:\", len(aiw_tokens))\n",
    "print(\"Sample:\", aiw_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_sents = sent_tokenize(aiw)\n",
    "print(\"Total sentences:\", len(aiw_sents))\n",
    "print(\"Sample:\", aiw_sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to know the number of words, not tokens (so excluding punctuation marks)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_words = [word for word in aiw_tokens if word.isalpha()]  # List comprehension\n",
    "print(\"Total words:\", len(aiw_words))\n",
    "### Same as: ###\n",
    "# aiw_words = []\n",
    "# for word in aiw_tokens:\n",
    "#     if word.isalpha():\n",
    "#         aiw_words += [word]  # same as aiw_words = aiw_words + [word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to know the size of the vocabulary, or the number of unique words, in Alice's Adventures in Wonderland?\n",
    "\n",
    "Remember that Python (and NLTK) consider capitalized and lowercased words to be different, so in order to count the number of unique words, we must **casefold** the text, changing all words to lowercase.  NLTK has a simple method for this: `.lower()`.  \n",
    "\n",
    "*Note:* Casefolding is a form of **normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_lower = [word.lower() for word in aiw_words]\n",
    "print(aiw_lower[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use another Python data type called a **set** to quickly get all the unique words from our list of casefolded words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_vocab = set(aiw_lower)\n",
    "print(\"Vocabulary size:\",len(aiw_vocab),\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other forms of **normalization** involve reducing words to their root form.  For example, the words \"happy\" and \"happiness\" have the same root and very similar meanings.  There are two ways NLTK provides to reduce words to their root form:\n",
    "\n",
    "* **Stemming**: reduces words to a root form where the root is *not* a valid word itself\n",
    "\n",
    "    In our example, the stem of \"happy\" and \"happiness\" would be \"happ.\"\n",
    "\n",
    "\n",
    "* **Lemmatizing**: reduces words to a root form where the root *is* a valid word itself, determined based on whether it exists in WordNet's list of valid English words\n",
    "\n",
    "    In our example, the stem of \"happy\" and \"happiness\" would be \"happy.\"\n",
    "    \n",
    "There are different approaches to stemming and lemmatization we can use in NLTK.  Let's see how they differ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(word) for word in aiw_lower]  # only includes alphabetic tokens\n",
    "print(porter_stemmed[500:550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in aiw_lower] # only includes alphabetic tokens\n",
    "print(lancaster_stemmed[500:550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(word) for word in aiw_lower]  # only includes alphabetic tokens\n",
    "print(lemmatized[500:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences do you spot in the output samples of stems and lemmas?\n",
    "\n",
    "*This is why it's always useful to print out samples of the data you're working with as you're coding!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we do with words once we've stemmed or lemmatized them?  Well, we could count the unique stems and lemmas, to get a different perspective on the size of the Lewis Carroll's vocabulary in Alice's Adventures in Wonderland, just as we did with the complete words.  \n",
    "\n",
    "We could also count the frequency at which these root forms of words appear, giving us a sense of what the most common words are in the book.  Let's try that!  We'll use NLTK's `FreqDist()` function (for calculating and visualizing frequency distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lemmas = FreqDist(lemmatized)\n",
    "fdist_lemmas  # pairs of lemmas and their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask how often a particular lemma appears using the `fdist_lemmas` variable we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist_lemmas['wonder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an easier overview, we can use visualization.  Let's create a line chart of the top 10 lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rc('font', size=12)\n",
    "number_of_tokens = 10 \n",
    "fdist_lemmas.plot(number_of_tokens, title='Frequency Distribution for '+str(number_of_tokens)+\" Most Common Lemmas in Alice's Adventures in Wonderland\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.  Some of these words don't tell us a lot.  It's pretty logical that words like \"the\" and \"she\" would be used a lot, but it doesn't tell us much about what goes on in the book.\n",
    "\n",
    "These small words that occur frequently but don't always carry large meaning, particularly in books and longer texts, are called **stopwords**.  We can filter them out with a `stopwords()` method and try re-plotting this frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = stopwords.words('english')  # english since the book is in English\n",
    "\n",
    "# What other words might we want to exclude?\n",
    "to_exclude += ['alice', \"said\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension to filter\n",
    "lemmatized_filtered = [l for l in lemmatized if (len(l) > 2 and not l in to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lemmas_filtered = FreqDist(lemmatized_filtered)\n",
    "print(\"Total words after filtering:\", fdist_lemmas_filtered.N())\n",
    "print(\"50 most common words after filtering:\", fdist_lemmas_filtered.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rc('font', size=12)\n",
    "number_of_tokens = 10 \n",
    "fdist_lemmas_filtered.plot(number_of_tokens, title='Frequency Distribution for '+str(number_of_tokens)+\" Most Common Lemmas in Alice's Adventures in Wonderland Excluding Stop Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more interesting!  We could do the same thing with complete words, to get a different perspective on the most common words in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common step to preprocessing text data is **part-of-speech (POS) tagging**.  POS tagging assigns parts of speech to words and groups of words in sentences.  After tagging parts of speech, you can perform more complex tasks such as **entity recognition**, which is the process of identifying people, places, and organizations named in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_tagged = nltk.pos_tag(aiw_tokens)\n",
    "print(aiw_tagged[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parts of speech the abbreviations stand for are available [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/home/dave/CodeToJoy/nltk-intro/nltk_data')\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
