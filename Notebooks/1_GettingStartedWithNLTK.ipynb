{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Natural Language Toolkit (NLTK)\n",
    "\n",
    "## Text Analysis with NLTK\n",
    "\n",
    "### [Centre for Data, Culture & Society](http://cdcs.ed.ac.uk)\n",
    "\n",
    "Course Instructor: Xandra Dave Cochran\n",
    "\n",
    "Course Dates: February 2024\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Refresher\n",
    "\n",
    "Can you name some of Python's **data types**?\n",
    "\n",
    "There are two data types that are particularly important to understand when doing text analysis: **string** and **list**.  \n",
    "\n",
    "**Strings** are sequences of characters (as in letters, numbers, spaces, or punctuation marks) that are contained within single quotes (`'I am a string'`) or double quotes (`\"I'm a string too!\"`).\n",
    "\n",
    "**Lists** are collections of data of any type (str, int, float, dict, list, tuple, set) that are contained within square brackets (`[ ]`).  Lists can have zero or more elements, and a single list can contain elements of different data types (for example: `['hi', 123, \"bonjour !\", [], {\"A\":1, \"B\":2}, (1.5, 2.8), {'a', 'b', 'c'}]`).  The elements inside lists are *ordered*, which allows us to reference them by their *index* (position - numeric, starts from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world\")   # print() is a FUNCTION, \"Hello world\" is a STRING (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = \"Hello world\"  # we've assigned the string to a VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment.  Comments are for you and me, Python knows to ignore them.\n",
    "# In this comment I'll explain that to show values, we can choose to use print() or \n",
    "# not, and the output will be slightly different, as you can see below.\n",
    "# If you want to show multiple values below a code cell like this one, you'll need all\n",
    "# of the values you want returned inside a print() function (except the last one, which\n",
    "# is optional), otherwise only the value in the last line of your cell will be shown\n",
    "print(greeting)\n",
    "greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_list = [\"hello\", \"bonjour\", \"salaam\", \"hola\"]  # 1-dimensional list of strings\n",
    "print(type(greeting_list))                              # type() is another function\n",
    "print(type(greeting_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a **for** loop or a **while** loop to iterate through items in a collection, such as our `greeting_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for greeting in greeting_list:\n",
    "    print(\"Greeting at index \"+str(i)+\": \"+greeting)  # an example of CONCATENTATION \n",
    "    i += 1                                            # a shortcut for writing i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "while i > -5:\n",
    "    print(greeting_list[i])  # we can reference indeces backwards using negative numbers\n",
    "    i = i - 1                # we can't write i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define functions in order to make our code easy to re-use and maintain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_greetings(gl):\n",
    "    for i, greeting in enumerate(gl): # you can iterate over the indices *and* the content of a list with the enumerate() function\n",
    "        print(f\"Greeting at index {i}: {greeting}\") # You can also insert values into strings with the f-string syntax\n",
    "\n",
    "show_greetings(greeting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclaim(greeting):\n",
    "    exclamation = greeting + '!'\n",
    "    # you can use conditionals to make code run only if a condition is met:\n",
    "    if greeting == 'hola':\n",
    "        # Use `return` when you want a function to return a value that can be used by other code\n",
    "        # (`print` displays the value, but doesn't make the it available to the code that called the function)\n",
    "        return 'ยก' + exclamation\n",
    "    else:\n",
    "        return exclamation\n",
    "    \n",
    "for g in greeting_list:\n",
    "    print(exclaim(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access subsets of a list using **slicing** with square brackets and colons, where the number before the colon is included in the slice but the number after the colon is not.  If a number is omitted before the colon, Python knows to go all the way to the starting element of the list.  If a number is omitted after the colon, Python knows to go all the way to the ending element of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greeting_list[0:1])\n",
    "print(greeting_list[:1])\n",
    "print(greeting_list[2:])\n",
    "print(greeting_list[:])\n",
    "print(greeting_list[:-3])\n",
    "print(greeting_list[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists are not the only built-in data-structure in Python. A `set` is an *unordered* data-structure that can hold a collection of *unique* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greetings_set = set(greeting_list)\n",
    "print(greetings_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_set = {'Leonardo', 'Donatello', 'Raphael', 'Michelangelo'}\n",
    "print(artist_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mere_oblivion_list = ['sans', 'teeth', 'sans', 'eyes', 'sans', 'taste', 'sans', 'everything']\n",
    "mere_oblivion_set = set(mere_oblivion_list)\n",
    "print(mere_oblivion_list)\n",
    "print(mere_oblivion_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis with NLTK\n",
    "\n",
    "****\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "Steven Bird, Ewan Klein, Edward Loper (2019) *Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit.*  3rd Edition.  https://www.nltk.org/book/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK, which stands for Natural Language Toolkit, is a popular coding library for text analysis with the programming language Python.  While Python alone has some basic capabilities for analyzing text, NLTK has much more to offer, as we will see!  This Jupyter Notebook will cover the following concepts:\n",
    "\n",
    "* [Tokenization](#tok)\n",
    "* [Frequency counts and distributions](#fre)\n",
    "* [Normalization](#nor)\n",
    "* [Stemming](#ste)\n",
    "* [Lemmatizing](#lem)\n",
    "* [Part-of-speech tagging](#pos)\n",
    "* [Collocations and n-grams](#col)\n",
    "\n",
    "These are the building blocks for more complicated text analysis tasks.  They are generally part of the first step of text analysis called **preprocessing**, which gets your text data formatted in a way that NLTK's methods and functions can easily interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin our text analysis, though, we should import the libraries we'll want to use to explore the capabilities of NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import * # the `*` means import all corpora (you could also specify a specific corpus)\n",
    "\n",
    "# As an alternative to `nltk.download()`, as shown in last Notebook's class, you can specify what\n",
    "# packages from NLTK to download\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet, gutenberg\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt\n",
    "\n",
    "import matplotlib.pyplot as plt   # for drawing charts to visualize data\n",
    "\n",
    "import re               # the Regular Expression (or RegEx) library, which is useful in combination with NLTK\n",
    "import string           # another useful library for acccessing lists of all letters, all punctuation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the names on the left (`text1`, `text2`, ..., `text9`) are **variables** that point to the text corpora on the right (`Moby Dick...`, `Sense and Senibility...`, ..., `The Man Who Was Thursday...`).\n",
    "\n",
    "I wonder what happens if we try to print one of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.  That doesn't show us much.  What if we try slicing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"tok\"></a>\n",
    "You can see that the text has been **tokenized**, or separated into its individual words, numbers, and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work out the length in tokens of `text1` and `text2`, *Moby Dick* and *Sense and Sensibility*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = len(text1)\n",
    "ss = len(text2)\n",
    "print(\"Length of Moby Dick:\", md)\n",
    "print(\"Length of Sense and Sensibility:\", ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to calculate the number of *unique* tokens in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(text):\n",
    "    pass # replace this with your own code\n",
    "\n",
    "print(\"Length of vocabulary (unique words) of Moby Dick:\", vocab_size(text1))\n",
    "print(\"Length of vocabulary (unique words) of Sense and Sensibility:\", vocab_size(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure the diversity of word choice, \n",
    "# or lexical diversity. This should be the ratio of vocabulary to length:\n",
    "def lexical_diversity(text):\n",
    "    pass\n",
    "\n",
    "print(\"Lexical Diversity of Moby Dick:\", lexical_diversity(text1))\n",
    "print(\"Lexical Diversity of Sense and Sensibility:\", lexical_diversity(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing works for us to view **tokens** in the NLTK Text object, which is the data type of `text2`.  Let's try out some of the methods and functions specific to NLTK, though, as they are designed for working with Text objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3\n",
    "\n",
    "Let's begin with the `concordance()` method.  We pass a single **token**, as a **string** (type=str), into this method.  What token would you like to see in its different contexts within the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('YOUR_TOKEN_HERE')  # optional parameter: lines=20 (or any number you choose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `concordance()` method shows 25 contexts in which the input word is used, but you can specify how many contexts you would like to see by saying something like: `text2.concordance('opinion', lines=63`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('opinion', lines=100)  # it's okay to input a greater number of lines than there are matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"Marianne\", lines=20)  # default lines listed is 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.similar(X)` will output words that appear in the text surrounded by similar words to `X`.  This means that the output is likely to include words that are the same part of speech as `X` and words that are synonyms of `X`.  For example, the words that occur in similar contexts to the noun `\"happiness\"` are also nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2.similar(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the concordances for 'kindness' and 'happiness'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"kindness\", lines=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty similar, huh? In text as in life, those two seem to be found in the same places...\n",
    "\n",
    "`.common_contexts(L, N)` will output words that appear immediately to the left and right of all input words in list `L`.  For example, `\"such\"` and `\"of\"` are found surrounding both `\"kindness\"` and `\"happiness\"` in text2, so they are included in the output of `.common_contexts([\"kindness\", \"happiness\"])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"], num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"], num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no result is found, NLTK will output a message that tells you this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"kindness\", \"happiness\"])  # defaults to 20 maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type the output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_and_happy = text2.common_contexts([\"kindness\", \"happiness\"])  # defaults to 20 maximum\n",
    "type(kind_and_happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a tad disquieting. It seems to reflect a reather bleak view of humankind.\n",
    "\n",
    "Of course, NLTK isn't really having existential angst, it's just that `Text.common_contexts` is not a function that `returns` a value, it just `prints` it: and if all you want to do with the output is look at it, I suppose that's fine: but if you want to save it to a variable or pass it to other code, you need it to `return`` the results, not `print` them. This can be done with `Text._word_context_index.common_contexts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_and_happy = text2._word_context_index.common_contexts([\"kindness\", \"happiness\"])  # defaults to 20 maximum\n",
    "print(type(kind_and_happy))\n",
    "print(kind_and_happy)\n",
    "kind_and_happy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 4\n",
    "\n",
    "To see the frequency of a list of words (of any length) as they occur from the beginning through the end of the text, you can use a Lexical Dispersion Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character names (note that they can't be multiple words, or there won't be a match)\n",
    "text2.dispersion_plot([\"Marianne\", \"Elinor\", \"Edward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this plot reflect the text?\n",
    "\n",
    "Marianne and Elinor are the main characters of the book 'Sense and Sensibility,' so it makes sense that we'd see them consistently throughout the text!  Edward is a supporting character, so we see that his name occurs less frequently.\n",
    "\n",
    "*waaaaaiiitasec* ... OK, this doesn't look right. Hang on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.dispersion_plot([\"the\", \"Edward\", \"cold\", \"gkjhjdgkkjdghdfkjghfk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Checks StackOverflow*\n",
    "\n",
    "Yeah, NLTK has a bug in it. I've raised the issue with the authors, but in the meantime, here is a function that should make it behave correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "\n",
    "def dispersion_plot_fixed(text, tokens):\n",
    "    # this doesn't work with text.dispersion_ploy, which \n",
    "    # displays the plot but doesn't return it: so instead \n",
    "    # I used `nltk.draw.dispersion.dispersion_plot`\n",
    "    ax = dispersion_plot(text, tokens) \n",
    "    ax.set_yticks(list(range(len(tokens))), reversed(tokens), color=\"C0\")\n",
    "\n",
    "dispersion_plot_fixed(text2, [\"the\", \"Edward\", \"cold\", \"gkjhjdgkkjdghdfkjghfk\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion_plot_fixed(text2, [\"Marianne\", \"Elinor\", \"Edward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better!\n",
    "\n",
    "Have a go yourself: find some words that show an interesting distribution - such that they tend to appear in different parts of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 5\n",
    "\n",
    "When we talk about the *context* of words (or tokens!) in text analysis, we're referring to the surrounding words of a given word.  Concordances show a bit of context to the left of an input word (just before the word appears) and to the right of that word (just after that word appeared). \n",
    "\n",
    "The words \"good\" and \"opinion\" seem to occur together quite a bit!  To see the other words appear near that pair, we can use the `common_contexts()` method.  We pass a **list** of tokens (each token as a string) into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"good\", \"opinion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that \"the good opinion of\" is the complete phrase in which the pair of words, \"good\" and \"opinion\", appear together in this text.  They don't occur together in other contexts!  But what about individually?\n",
    "\n",
    "We can use the **similar** method to see words that appear in similar contexts, meaning they're surrounded by similar tokens, as the token we input.  Note that we pass in a single token as a string to this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words with a similar context as 'good':\")\n",
    "text2.similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words with a similar context as 'opinion':\")\n",
    "text2.similar(\"opinion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of words that occur together, such as \"good\" and \"opinion,\" are referred to as **bigrams**, where \"bi\" indicates two.  **N-grams** are groups of words that occur together, where n is a number of your choice.\n",
    "\n",
    "To get all the bigrams in a text, we can use the `bigrams()` method, into which we pass the variable referring to the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_list = list(nltk.bigrams(text2))\n",
    "print(bigrams_list[:100])  # prints the first 100 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class we looked quickly at a **dispersion plot**, which is a chart that visualizes when particular tokens appear within a text.  Let's try making another one of those.  We pass in a list of individual tokens, where each token is a string, to make a dispersion plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another text.  NLTK includes some books that were digitized for [Project Gutenberg](https://www.gutenberg.org).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 6\n",
    "\n",
    "Let's look at one of those books, Alice's Adventures in Wonderland (with the fileid `carroll-alice.txt`), to practice tokenizing on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")\n",
    "print(type(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize the string of Alice's Adventures in Wonderland to split it into individual words and punctuation using the function `word_tokenize()`.  We can split the string into individual sentences using the function `sent_tokenize()`.  Both tokenization functions output a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_tokens = word_tokenize(alice)\n",
    "print(\"Total tokens:\", len(alice_tokens))\n",
    "print(\"Sample:\", alice_tokens[0:100])\n",
    "print(type(alice_tokens))\n",
    "print(type(alice_tokens[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the output of the tokenizer isn't a special NLTK type: it's just a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sents = sent_tokenize(alice)\n",
    "print(\"Total sentences:\", len(alice_sents))\n",
    "print(\"Sample:\", alice_sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to know the number of words, not tokens (so excluding punctuation marks)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(tokens):\n",
    "    return [word for word in alice_tokens if word.isalpha()]  # List comprehension\n",
    "    # If you don't know ab out list comprehensions, stop me and ask about list comprehensions!\n",
    "\n",
    "alice_words = get_words(alice)\n",
    "print(\"Total words:\", len(alice_words))\n",
    "### Same as: ###\n",
    "# alice_words = []\n",
    "# for word in alice_tokens:\n",
    "#     if word.isalpha():\n",
    "#         alice_words += [word]  # same as alice_words = alice_words + [word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to know the size of the vocabulary, or the number of unique words, in Alice's Adventures in Wonderland?\n",
    "\n",
    "Remember that Python (and NLTK) consider capitalized and lowercased words to be different, so in order to count the number of unique words, we must **casefold** the text, changing all words to lowercase.  Python strings have a simple method for this: `.lower()`. Write a function which casefolds a list of words, and pass `alice_words` to it.\n",
    "\n",
    "*Note:* Casefolding is a form of **normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_words(words):\n",
    "    pass\n",
    "\n",
    "alice_lower = lower_words(alice_words)\n",
    "print(alice_lower[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's count all the unique words from our list of casefolded words:\n",
    "\n",
    "*Note*: Conveniently, we already wrote a function to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"Vocabulary size:\", alice_vocab_size, \"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 7\n",
    "\n",
    "Other forms of **normalization** involve reducing words to their root form.  For example, the words \"happy\" and \"happiness\" have the same root and very similar meanings.  There are two ways NLTK provides to reduce words to their root form:\n",
    "\n",
    "* **Stemming**: reduces words to a root form where the root is *not* a valid word itself\n",
    "\n",
    "    In our example, the stem of \"happy\" and \"happiness\" would be \"happ.\"\n",
    "\n",
    "\n",
    "* **Lemmatizing**: reduces words to a root form where the root *is* a valid word itself, determined based on whether it exists in WordNet's list of valid English words\n",
    "\n",
    "    In our example, the stem of \"happy\" and \"happiness\" would be \"happy.\"\n",
    "    \n",
    "There are different approaches to stemming and lemmatization we can use in NLTK.  Let's see how they differ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(word) for word in alice_lower]  # only includes alphabetic tokens\n",
    "print(porter_stemmed[500:550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in alice_lower] # only includes alphabetic tokens\n",
    "print(lancaster_stemmed[500:550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(word) for word in alice_lower]  # only includes alphabetic tokens\n",
    "print(lemmatized[500:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences do you spot in the output samples of stems and lemmas?\n",
    "\n",
    "*This is why it's always useful to print out samples of the data you're working with as you're coding!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we do with words once we've stemmed or lemmatized them?  Well, we could count the unique stems and lemmas, to get a different perspective on the size of the Lewis Carroll's vocabulary in Alice's Adventures in Wonderland, just as we did with the complete words.  \n",
    "\n",
    "We could also count the frequency at which these root forms of words appear, giving us a sense of what the most common words are in the book.  Let's try that!  We'll use NLTK's `FreqDist()` function (for calculating and visualizing frequency distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lemmas = FreqDist(lemmatized)\n",
    "fdist_lemmas  # pairs of lemmas and their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask how often a particular lemma appears using the `fdist_lemmas` variable we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist_lemmas['wonder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an easier overview, we can use visualization.  Let's create a line chart of the top 10 lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rc('font', size=12)\n",
    "number_of_tokens = 10 \n",
    "fdist_lemmas.plot(number_of_tokens, title='Frequency Distribution for '+str(number_of_tokens)+\" Most Common Lemmas in Alice's Adventures in Wonderland\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.  Some of these words don't tell us a lot.  It's pretty logical that words like \"the\" and \"she\" would be used a lot, but it doesn't tell us much about what goes on in the book.\n",
    "\n",
    "These small words that occur frequently but don't always carry large meaning, particularly in books and longer texts, are called **stopwords**.  We can filter them out with a `stopwords()` method and try re-plotting this frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = stopwords.words('english')  # english since the book is in English\n",
    "\n",
    "# What other words might we want to exclude?\n",
    "to_exclude += ['alice', \"said\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function (hint: use a list comprehension) to filter out stop words and words shorter than 2 letters\n",
    "def filter_lemmas(lemmas, stops):\n",
    "    pass\n",
    "\n",
    "lemmatized_filtered = filter_lemmas(lemmatized, to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lemmas_filtered = FreqDist(lemmatized_filtered)\n",
    "print(\"Total words after filtering:\", fdist_lemmas_filtered.N())\n",
    "print(\"50 most common words after filtering:\", fdist_lemmas_filtered.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rc('font', size=12)\n",
    "number_of_tokens = 10 \n",
    "fdist_lemmas_filtered.plot(number_of_tokens, title='Frequency Distribution for '+str(number_of_tokens)+\" Most Common Lemmas in Alice's Adventures in Wonderland Excluding Stop Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more interesting!  We could do the same thing with complete words, to get a different perspective on the most common words in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common step to preprocessing text data is **part-of-speech (POS) tagging**.  POS tagging assigns parts of speech to words and groups of words in sentences.  After tagging parts of speech, you can perform more complex tasks such as **entity recognition**, which is the process of identifying people, places, and organizations named in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_tagged = nltk.pos_tag(alice_tokens)\n",
    "print(alice_tagged[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parts of speech the abbreviations stand for are available [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
